<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Audio Mobile IA</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/speech-commands@latest/dist/speech-commands.min.js"></script>
</head>
<body style="font-family: sans-serif; text-align: center; padding: 20px;">

    <h2>Classificateur Audio Mobile</h2>
    
    <button id="start-btn" style="padding: 15px 30px; font-size: 18px; cursor: pointer;">
        ðŸŽ¤ Activer le Micro & Lancer
    </button>

    <div id="status" style="margin-top: 20px; color: gray;">En attente...</div>
    <div id="result" style="margin-top: 20px; font-weight: bold; font-size: 24px;"></div>

    <script>
        const URL = "https://teachablemachine.withgoogle.com/models/33EVLsQd_/"; // <--- METS TON LIEN ICI
        let recognizer;

        async function createModel() {
            const checkpointURL = URL + "model.json";
            const metadataURL = URL + "metadata.json";

            const recognizer = speechCommands.create(
                "BROWSER_FFT",
                undefined,
                checkpointURL,
                metadataURL
            );

            await recognizer.ensureModelLoaded();
            return recognizer;
        }

        document.getElementById('start-btn').addEventListener('click', async () => {
            const status = document.getElementById('status');
            const resultDiv = document.getElementById('result');
            
            status.innerText = "Chargement du modÃ¨le...";

            try {
                // 1. Initialisation du modÃ¨le
                if (!recognizer) {
                    recognizer = await createModel();
                }

                // 2. Gestion spÃ©cifique au Mobile : Forcer l'AudioContext
                // On rÃ©cupÃ¨re l'AudioContext utilisÃ© par le recognizer
                const audioContext = tf.backend().audioContext || new (window.AudioContext || window.webkitAudioContext)();
                
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                status.innerText = "Ã‰coute en cours...";

                // 3. Lancement de la reconnaissance
                const classLabels = recognizer.wordLabels();
                recognizer.listen(result => {
                    const scores = result.scores;
                    // Trouver l'index du score le plus Ã©levÃ©
                    const maxScore = Math.max(...scores);
                    const index = scores.indexOf(maxScore);
                    
                    resultDiv.innerText = `${classLabels[index]} (${(maxScore * 100).toFixed(0)}%)`;
                }, {
                    includeSpectrogram: false,
                    probabilityThreshold: 0.75,
                    invokeCallbackOnNoiseAndUnknown: true,
                    overlapFactor: 0.5
                });

            } catch (err) {
                status.innerText = "Erreur : " + err.message;
                console.error(err);
            }
        });
    </script>
</body>
</html>
