<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <title>Reconnaissance Hiragana - Whisper AI</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    body {
      font-family: system-ui, sans-serif;
      text-align: center;
      padding: 2rem;
      max-width: 600px;
      margin: 0 auto;
    }
    button {
      font-size: 1.2rem;
      padding: 1rem 2rem;
      cursor: pointer;
      border: 2px solid #333;
      border-radius: 8px;
      background: #4CAF50;
      color: white;
      font-weight: bold;
      transition: all 0.3s;
    }
    button:hover:not(:disabled) {
      background: #45a049;
    }
    button:disabled {
      background-color: #ccc;
      border-color: #999;
      cursor: not-allowed;
    }
    button:active:not(:disabled) {
      transform: scale(0.98);
    }
    #kana {
      font-size: 4rem;
      margin-top: 1.5rem;
      min-height: 5rem;
    }
    #raw {
      margin-top: 1rem;
      color: #666;
      font-size: 0.9rem;
    }
    #error {
      margin-top: 1rem;
      color: red;
      font-size: 1rem;
      padding: 0.5rem;
    }
    #loading {
      margin-top: 2rem;
      padding: 1.5rem;
      background: #f0f8ff;
      border-radius: 8px;
      border: 2px solid #4CAF50;
    }
    #loading.hidden {
      display: none;
    }
    .progress-bar {
      width: 100%;
      height: 20px;
      background: #e0e0e0;
      border-radius: 10px;
      overflow: hidden;
      margin: 10px 0;
    }
    .progress-fill {
      height: 100%;
      background: linear-gradient(90deg, #4CAF50, #45a049);
      width: 0%;
      transition: width 0.3s;
    }
    #loadingText {
      font-size: 0.9rem;
      color: #333;
      margin-top: 0.5rem;
    }
    #debug {
      margin-top: 2rem;
      padding: 1rem;
      background: #f5f5f5;
      border-radius: 8px;
      font-size: 0.85rem;
      text-align: left;
    }
    .success {
      color: green;
    }
    .recording-indicator {
      display: inline-block;
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: red;
      margin-right: 8px;
      animation: pulse 1s infinite;
    }
    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.3; }
    }
    #audioSection {
      margin-top: 2rem;
      padding: 1.5rem;
      background: #f8f9fa;
      border-radius: 10px;
      border: 2px solid #4CAF50;
    }
    #audioSection.hidden {
      display: none;
    }
    #audioSection h3 {
      margin-top: 0;
      color: #333;
      font-size: 1.1rem;
    }
    audio {
      width: 100%;
      margin: 1rem 0;
    }
    .btn-download {
      background: #2196F3;
      margin-top: 0.5rem;
    }
    .btn-download:hover:not(:disabled) {
      background: #1976D2;
    }
    .audio-info {
      font-size: 0.9rem;
      color: #666;
      margin-top: 0.5rem;
    }
  </style>
</head>
<body>
  <h1>üé§ Prononce un hiragana</h1>
  <p>Dis un son en japonais (a, ka, shi, etc.)</p>
  
  <!-- Indicateur de chargement -->
  <div id="loading">
    <div style="font-weight: bold; font-size: 1.1rem;">‚è≥ Chargement du mod√®le Whisper...</div>
    <div class="progress-bar">
      <div id="progressFill" class="progress-fill"></div>
    </div>
    <div id="loadingText">Initialisation...</div>
  </div>
  
  <button id="mic" disabled>Parler</button>
  <div id="kana">‚Äî</div>
  <div id="raw"></div>
  <div id="error"></div>
  
  <!-- Section Audio Player -->
  <div id="audioSection" class="hidden">
    <h3>üîä Audio enregistr√©</h3>
    <audio id="audioPlayer" controls></audio>
    <div class="audio-info">
      Dur√©e: <span id="audioDuration">-</span>s | 
      Taille: <span id="audioSize">-</span>
    </div>
    <button class="btn-download" onclick="downloadAudio()">‚¨áÔ∏è T√©l√©charger WAV</button>
  </div>
  
  <div id="debug"></div>

  <!-- Charger Transformers.js -->
  <script type="module">
    import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2';
    
    // Configuration
    env.allowLocalModels = false;
    
    // Variables globales
    window.whisperPipeline = null;
    window.isWhisperReady = false;
    
    // Fonction de debug
    function addDebug(msg, isError = false) {
      const time = new Date().toLocaleTimeString();
      const debugDiv = document.getElementById('debug');
      debugDiv.innerHTML += `<div style="color: ${isError ? 'red' : '#333'}">[${time}] ${msg}</div>`;
    }
    
    // Charger le mod√®le Whisper
    async function loadWhisper() {
      try {
        document.getElementById('loadingText').textContent = 
          'T√©l√©chargement du mod√®le Whisper (tiny) - ~75MB...';
        document.getElementById('progressFill').style.width = '10%';
        
        addDebug('D√©but du chargement de Whisper...');
        
        window.whisperPipeline = await pipeline(
          'automatic-speech-recognition',
          'Xenova/whisper-small',
          {
            progress_callback: (progress) => {
              if (progress.status === 'progress') {
                const percent = Math.round((progress.loaded / progress.total) * 100);
                document.getElementById('progressFill').style.width = percent + '%';
                document.getElementById('loadingText').textContent = 
                  `T√©l√©chargement: ${percent}%`;
              }
            }
          }
        );
        
        document.getElementById('progressFill').style.width = '100%';
        document.getElementById('loadingText').textContent = '‚úÖ Mod√®le charg√© !';
        
        setTimeout(() => {
          document.getElementById('loading').classList.add('hidden');
        }, 1000);
        
        document.getElementById('mic').disabled = false;
        
        window.isWhisperReady = true;
        addDebug('‚úÖ Whisper pr√™t √† l\'utilisation');
        
      } catch (error) {
        addDebug('‚ùå Erreur chargement Whisper: ' + error.message, true);
        document.getElementById('error').textContent = 
          '‚ùå Erreur de chargement: ' + error.message;
      }
    }
    
    // D√©marrer le chargement
    addDebug('Script charg√©');
    loadWhisper();
  </script>

  <script>
    const micBtn = document.getElementById("mic");
    const kanaDiv = document.getElementById("kana");
    const rawDiv = document.getElementById("raw");
    const errorDiv = document.getElementById("error");
    const debugDiv = document.getElementById("debug");

    // Variables pour l'enregistrement
    let mediaRecorder = null;
    let audioChunks = [];
    let lastRecordedBlob = null;
    const RECORDING_DURATION = 3000; // 3 secondes

    // Fonction de debug
    function addDebug(msg, isError = false) {
      const time = new Date().toLocaleTimeString();
      debugDiv.innerHTML += `<div style="color: ${isError ? 'red' : '#333'}">[${time}] ${msg}</div>`;
    }

    // Mapping hiragana √©tendu
    const kanaMap = {
      "a": "„ÅÇ", "i": "„ÅÑ", "u": "„ÅÜ", "e": "„Åà", "o": "„Åä",
      "ka": "„Åã", "ki": "„Åç", "ku": "„Åè", "ke": "„Åë", "ko": "„Åì",
      "sa": "„Åï", "shi": "„Åó", "si": "„Åó",
      "su": "„Åô", "se": "„Åõ", "so": "„Åù",
      "ta": "„Åü", "chi": "„Å°", "ti": "„Å°",
      "tsu": "„Å§", "tu": "„Å§",
      "te": "„Å¶", "to": "„Å®",
      "na": "„Å™", "ni": "„Å´", "nu": "„Å¨", "ne": "„Å≠", "no": "„ÅÆ",
      "ha": "„ÅØ", "hi": "„Å≤", "fu": "„Åµ", "hu": "„Åµ",
      "he": "„Å∏", "ho": "„Åª",
      "ma": "„Åæ", "mi": "„Åø", "mu": "„ÇÄ", "me": "„ÇÅ", "mo": "„ÇÇ",
      "ya": "„ÇÑ", "yu": "„ÇÜ", "yo": "„Çà",
      "ra": "„Çâ", "ri": "„Çä", "ru": "„Çã", "re": "„Çå", "ro": "„Çç",
      "wa": "„Çè", "wo": "„Çí", "n": "„Çì",
      "ji": "„Åò", "zi": "„Åò",
      "ga": "„Åå", "gi": "„Åé", "gu": "„Åê", "ge": "„Åí", "go": "„Åî",
      "za": "„Åñ", "zu": "„Åö", "ze": "„Åú", "zo": "„Åû",
      "da": "„Å†", "de": "„Åß", "do": "„Å©",
      "ba": "„Å∞", "bi": "„Å≥", "bu": "„Å∂", "be": "„Åπ", "bo": "„Åº",
      "pa": "„Å±", "pi": "„Å¥", "pu": "„Å∑", "pe": "„Å∫", "po": "„ÅΩ"
    };

    // Fonction de reconnaissance avec Whisper
    async function recognizeWithWhisper(audioBlob) {
      if (!window.isWhisperReady) {
        throw new Error('Whisper pas encore charg√©');
      }
      
      try {
        const arrayBuffer = await audioBlob.arrayBuffer();
        
        // Cr√©er AudioContext sans sp√©cifier le sampleRate
        let audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        
        // Resample √† 16kHz pour Whisper
        const targetSampleRate = 16000;
        let audioData;
        
        if (audioBuffer.sampleRate === targetSampleRate) {
          audioData = audioBuffer.getChannelData(0);
        } else {
          const offlineContext = new OfflineAudioContext(
            1,
            audioBuffer.duration * targetSampleRate,
            targetSampleRate
          );
          
          const source = offlineContext.createBufferSource();
          source.buffer = audioBuffer;
          source.connect(offlineContext.destination);
          source.start();
          
          const resampledBuffer = await offlineContext.startRendering();
          audioData = resampledBuffer.getChannelData(0);
        }
        
        await audioContext.close();
        
        // Transcription avec Whisper
        const result = await window.whisperPipeline(audioData, {
          language: 'japanese',
          task: 'transcribe'
        });
        
        return result.text;
        
      } catch (error) {
        addDebug('Erreur reconnaissance: ' + error.message, true);
        throw error;
      }
    }

    // D√©marrer l'enregistrement
    async function startRecording() {
      try {
        addDebug('D√©but de l\'enregistrement');
        kanaDiv.innerHTML = '<span class="recording-indicator"></span>üéß √âcoute...';
        rawDiv.textContent = "";
        errorDiv.textContent = "";
        micBtn.disabled = true;
        micBtn.textContent = "√âcoute en cours...";
        
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 48000
          }
        });
        
        // D√©tecter le meilleur format
        let mimeType = 'audio/webm';
        if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
          mimeType = 'audio/webm;codecs=opus';
        } else if (MediaRecorder.isTypeSupported('audio/mp4')) {
          mimeType = 'audio/mp4';
        }
        
        addDebug('Format audio: ' + mimeType);
        
        mediaRecorder = new MediaRecorder(stream, { 
          mimeType: mimeType,
          audioBitsPerSecond: 128000
        });
        audioChunks = [];
        
        mediaRecorder.ondataavailable = (event) => {
          audioChunks.push(event.data);
        };
        
        mediaRecorder.onstop = async () => {
          stream.getTracks().forEach(track => track.stop());
          await processRecording();
        };
        
        mediaRecorder.start();
        addDebug('MediaRecorder d√©marr√©');
        
        setTimeout(() => {
          if (mediaRecorder && mediaRecorder.state === 'recording') {
            mediaRecorder.stop();
            addDebug('Enregistrement termin√©');
          }
        }, RECORDING_DURATION);
        
      } catch (error) {
        addDebug('Erreur enregistrement: ' + error.message, true);
        errorDiv.textContent = `Erreur: ${error.message}`;
        micBtn.disabled = false;
        micBtn.textContent = "Parler";
        kanaDiv.textContent = "‚Äî";
      }
    }

    // Traiter l'enregistrement
    async function processRecording() {
      try {
        kanaDiv.textContent = "‚öôÔ∏è Analyse...";
        addDebug('Traitement de l\'audio');
        
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        lastRecordedBlob = audioBlob;
        
        // Afficher les infos de l'audio
        const sizeKB = (audioBlob.size / 1024).toFixed(1);
        document.getElementById('audioSize').textContent = sizeKB + ' KB';
        
        // Convertir en WAV pour le lecteur et afficher la dur√©e
        const wavBlob = await convertToWav(audioBlob);
        const audioDuration = await getAudioDuration(wavBlob);
        document.getElementById('audioDuration').textContent = audioDuration.toFixed(2);
        
        // Afficher le lecteur audio
        const audioPlayer = document.getElementById('audioPlayer');
        audioPlayer.src = URL.createObjectURL(wavBlob);
        document.getElementById('audioSection').classList.remove('hidden');
        
        addDebug(`Audio: ${audioDuration.toFixed(2)}s, ${sizeKB}KB`);
        
        // Reconnaissance avec Whisper
        const transcript = await recognizeWithWhisper(audioBlob);
        
        addDebug(`Transcription: "${transcript}"`);
        
        const said = transcript.toLowerCase().trim();
        rawDiv.textContent = `Reconnu : "${said}"`;
        
        // Rechercher la correspondance
        let found = false;
        for (const [romaji, char] of Object.entries(kanaMap)) {
          if (
            said === romaji ||
            said === `c'est ${romaji}` ||
            said === `s√© ${romaji}` ||
            said === `${romaji} desu` ||
            said === `hiragana ${romaji}` ||
            said.endsWith(romaji) ||
            said.includes(romaji)
          ) {
            kanaDiv.textContent = char;
            found = true;
            addDebug(`‚úÖ Match trouv√©: ${romaji} ‚Üí ${char}`);
            break;
          }
        }
        
        if (!found) {
          kanaDiv.textContent = "‚ùì Pas reconnu";
          addDebug('‚ùå Aucune correspondance trouv√©e');
        }
        
      } catch (error) {
        addDebug('Erreur traitement: ' + error.message, true);
        errorDiv.textContent = `‚ùå ${error.message}`;
        kanaDiv.textContent = "‚Äî";
      } finally {
        micBtn.disabled = false;
        micBtn.textContent = "Parler";
      }
    }

    // Obtenir la dur√©e de l'audio
    async function getAudioDuration(blob) {
      return new Promise((resolve, reject) => {
        const audio = new Audio();
        audio.onloadedmetadata = () => {
          resolve(audio.duration);
        };
        audio.onerror = reject;
        audio.src = URL.createObjectURL(blob);
      });
    }

    // Convertir WebM en WAV
    async function convertToWav(webmBlob) {
      const arrayBuffer = await webmBlob.arrayBuffer();
      const audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      
      // Convertir en WAV
      const wavBlob = await audioBufferToWavBlob(audioBuffer);
      await audioContext.close();
      
      return wavBlob;
    }

    // Convertir AudioBuffer en WAV Blob
    async function audioBufferToWavBlob(audioBuffer) {
      const numberOfChannels = audioBuffer.numberOfChannels;
      const length = audioBuffer.length * numberOfChannels * 2;
      const buffer = new ArrayBuffer(44 + length);
      const view = new DataView(buffer);
      const channels = [];
      let offset = 0;
      let pos = 0;
      
      const setUint16 = (data) => {
        view.setUint16(pos, data, true);
        pos += 2;
      };
      const setUint32 = (data) => {
        view.setUint32(pos, data, true);
        pos += 4;
      };
      
      setUint32(0x46464952); // "RIFF"
      setUint32(length + 36);
      setUint32(0x45564157); // "WAVE"
      setUint32(0x20746d66); // "fmt "
      setUint32(16);
      setUint16(1);
      setUint16(numberOfChannels);
      setUint32(audioBuffer.sampleRate);
      setUint32(audioBuffer.sampleRate * numberOfChannels * 2);
      setUint16(numberOfChannels * 2);
      setUint16(16);
      setUint32(0x61746164); // "data"
      setUint32(length);
      
      for (let i = 0; i < audioBuffer.numberOfChannels; i++) {
        channels.push(audioBuffer.getChannelData(i));
      }
      
      offset = 44;
      for (let i = 0; i < audioBuffer.length; i++) {
        for (let channel = 0; channel < numberOfChannels; channel++) {
          let sample = Math.max(-1, Math.min(1, channels[channel][i]));
          sample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
          view.setInt16(offset, sample, true);
          offset += 2;
        }
      }
      
      return new Blob([buffer], { type: 'audio/wav' });
    }

    // T√©l√©charger l'audio
    function downloadAudio() {
      if (!lastRecordedBlob) {
        alert('‚ùå Aucun audio enregistr√©');
        return;
      }
      
      convertToWav(lastRecordedBlob).then(wavBlob => {
        const url = URL.createObjectURL(wavBlob);
        const a = document.createElement('a');
        a.href = url;
        a.download = `hiragana_recording_${Date.now()}.wav`;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);
      });
    }

    // Event listener sur le bouton
    micBtn.onclick = startRecording;
  </script>
</body>
</html>

